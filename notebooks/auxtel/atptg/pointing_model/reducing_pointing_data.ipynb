{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducing pointing data\n",
    "\n",
    "During the SUMMIT-4829 run, we obtained data at lower elevation to improve the existing pointing model. \n",
    "\n",
    "The data was taken in a slightly different way. Instead of centering the start in the CCD and storing the data for the pointing model we simply ran the wavefront analysis to collimate the optics, registered the position in the pointing and took an acquisition image, so we could later measure the offset and apply that to the registered position.\n",
    "\n",
    "This notebooks is intended to gather the information about the data taken during a run and store it in a file that can be read and processed offline to generate pointing files.\n",
    "These pointing files can then be used with `tpoint` to compute pointing models.\n",
    "\n",
    "The next step will be to measure the position of the brightest start in the field, compute the offset with respect to the center of the field and add that to the data generated by the pointing. \n",
    "This will be done on a separate notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameterized notebook\n",
    "\n",
    "This notebook is parameterized, which means it could be run with tools like Papermill as part of a data analysis pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "In order to run this notebook you will need an updated version of `QuickFrameMeasurementTask`. \n",
    "By the time of this writting this library was still not integrated to the DM stack, so you may need to set it up manually. \n",
    "\n",
    "Assuming you are in one of the nublado environments (tts, nts, summit, etc), open a terminal and do the following:\n",
    "\n",
    "```console\n",
    "source ${LOADSTACK}\n",
    "cd ${HOME}/WORK\n",
    "git clone https://github.com/lsst-sitcom/rapid_analysis.git\n",
    "cd rapid_analysis/\n",
    "eups declare -r . -t $USER\n",
    "```\n",
    "\n",
    "Then, on your `${HOME}/notebooks/.user_setups` file, add the following like:\n",
    "\n",
    "```\n",
    "setup rapid_analysis -t $USER\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from astropy.time import Time\n",
    "from astropy import units as u\n",
    "from datetime import timedelta, datetime\n",
    "\n",
    "import lsst_efd_client\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "\n",
    "from lsst.geom import PointD\n",
    "\n",
    "from lsst.pipe.tasks.quickFrameMeasurement import QuickFrameMeasurementTask\n",
    "\n",
    "from lsst.ts.observing.utilities.auxtel.latiss.getters import get_image\n",
    "from lsst.ts.observing.utilities.auxtel.latiss.utils import (\n",
    "    calculate_xy_offsets,\n",
    "    parse_obs_id,\n",
    ")\n",
    "from lsst.ts.observatory.control.constants.latiss_constants import boresight #, pixel_scale\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsst_efd_client.EfdClient.list_efd_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = lsst_efd_client.EfdClient('ldf_stable_efd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Parameters\n",
    "\n",
    "The next cell define the notebook parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date of the observation\n",
    "\n",
    "Basically the notebook take a year/month/day and time-window. It will then use that time spam to look for pointing data and associated images to analyse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "year=2021\n",
    "month=6\n",
    "day=9\n",
    "time_window=3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define timestamp when the data was taken\n",
    "\n",
    "The next cell defines the dates when the data was taken, it is used by tthe following query to determine when to look for pointing component data registration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SUMMIT-4829\n",
    "\n",
    "```\n",
    "start = Time('2021-02-18T00:00:00')\n",
    "end = Time('2021-02-20T00:00:00')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SUMMIT-5025\n",
    "\n",
    "```\n",
    "start = Time('2021-03-22T00:00:00')\n",
    "end = Time('2021-03-25T00:00:00')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = Time(f\"{year}-{month:02d}-{day:02d}T00:00:00\")\n",
    "end = Time(f\"{year}-{month:02d}-{day+time_window:02d}T00:00:00\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(start, end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data analysis\n",
    "\n",
    "Look from when pointAddData command was sent to the pointing. These will mark the times when we registered the positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = f\"time >= '{start}+00:00' AND time <= '{end}+00:00'\"\n",
    "query = f'SELECT \"expectedAzimuth\", \"expectedElevation\", \"measuredAzimuth\", \"measuredElevation\", \"measuredRotator\" '\\\n",
    "        f'FROM \"efd\".\"autogen\".\"lsst.sal.ATPtg.logevent_pointData\" WHERE {timestamp}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_data = await client.influx_client.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(point_data) == 0:\n",
    "    raise RuntimeError(f\"No pointing data in the specified time window: {start} - {end}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure ATAOS corrections offsets\n",
    "\n",
    "The offsets with respect to the hexapod x/y corrections are mapped into az/el corrections later. Since this is an asynchronous event, we need to query for all instances in the time window we are analysing and then match each entry with the associated image/pointing data taken. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ataos_correction_offsets = await client.select_time_series('lsst.sal.ATAOS.logevent_correctionOffsets', \n",
    "                                            [\"x\", \"y\", \"z\"], \n",
    "                                            start.tai, \n",
    "                                            end.tai)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Found {len(ataos_correction_offsets)} ATAOS correctionOffsets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding acquisition images\n",
    "\n",
    "Now we have the timestamps for when the telescope position was registered, we need to find the acquisition images.\n",
    "\n",
    "The images where taken before registering the position so we need to look ~40s before the command was sent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acq_image_name = []\n",
    "elevation = []\n",
    "rotator_1 = []\n",
    "rotator_2 = []\n",
    "hexapod_x = []\n",
    "hexapod_y = []\n",
    "hexapod_z = []\n",
    "ataos_correction_x = []\n",
    "ataos_correction_y = []\n",
    "ataos_correction_z = []\n",
    "\n",
    "\n",
    "for time_reg in point_data.index:\n",
    "    \n",
    "    imageName = await client.select_time_series('lsst.sal.ATCamera.logevent_endReadout', \n",
    "                                            [\"imageName\"], \n",
    "                                            Time(time_reg).tai - timedelta(seconds=40), \n",
    "                                            Time(time_reg).tai)    \n",
    "\n",
    "    print(imageName)\n",
    "    # It may happen that a image is not taken (or fails to be taken) when we register the position.\n",
    "    # In this cases, add `None` to the acq_obsid list. These data will later be ignored.\n",
    "    if hasattr(imageName, \"imageName\"):\n",
    "        acq_image_name.append(imageName[\"imageName\"][-1])\n",
    "    else:\n",
    "        acq_obsid.append(None)\n",
    "\n",
    "    mount_Nasmyth_Encoders = await client.select_packed_time_series(\"lsst.sal.ATMCS.mount_Nasmyth_Encoders\",\n",
    "                                                                    [\"nasmyth1CalculatedAngle\", \n",
    "                                                                     \"nasmyth2CalculatedAngle\"],\n",
    "                                                                    Time(time_reg).tai - timedelta(seconds=5), \n",
    "                                                                    Time(time_reg).tai)\n",
    "    rotator_1.append(np.mean(mount_Nasmyth_Encoders[\"nasmyth1CalculatedAngle\"]))\n",
    "    rotator_2.append(np.mean(mount_Nasmyth_Encoders[\"nasmyth2CalculatedAngle\"]))\n",
    "    \n",
    "    elevationCalculatedAngle = await client.select_packed_time_series(\"lsst.sal.ATMCS.mount_AzEl_Encoders\",\n",
    "                                                                      [\"elevationCalculatedAngle\"],\n",
    "                                                                    Time(time_reg).tai - timedelta(seconds=5), \n",
    "                                                                    Time(time_reg).tai\n",
    "                                                                     )\n",
    "    elevation.append(np.mean(elevationCalculatedAngle[\"elevationCalculatedAngle\"]))\n",
    "    \n",
    "    hexapod_positions = await client.select_time_series(\n",
    "        'lsst.sal.ATHexapod.positionStatus',\n",
    "        [\n",
    "            \"reportedPosition0\",\n",
    "            \"reportedPosition1\",\n",
    "            \"reportedPosition2\"\n",
    "        ],\n",
    "        Time(time_reg).tai - timedelta(seconds=40),\n",
    "        Time(time_reg).tai)\n",
    "    \n",
    "    hexapod_x.append(np.mean(hexapod_positions[\"reportedPosition0\"]))\n",
    "    hexapod_y.append(np.mean(hexapod_positions[\"reportedPosition1\"]))\n",
    "    hexapod_z.append(np.mean(hexapod_positions[\"reportedPosition2\"]))\n",
    "    \n",
    "    indx = ataos_correction_offsets.index.get_loc(point_data.index[0], method='nearest')\n",
    "\n",
    "    ataos_correction_x.append(ataos_correction_offsets[\"x\"][indx])\n",
    "    ataos_correction_y.append(ataos_correction_offsets[\"y\"][indx])\n",
    "    ataos_correction_z.append(ataos_correction_offsets[\"z\"][indx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ataos_correction_x = []\n",
    "ataos_correction_y = []\n",
    "ataos_correction_z = []\n",
    "\n",
    "for time_reg in point_data.index:\n",
    "\n",
    "    indx = ataos_correction_offsets.index.get_loc(time_reg, method='nearest')\n",
    "\n",
    "    ataos_correction_x.append(ataos_correction_offsets[\"x\"][indx])\n",
    "    ataos_correction_y.append(ataos_correction_offsets[\"y\"][indx])\n",
    "    ataos_correction_z.append(ataos_correction_offsets[\"z\"][indx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring star position in each image\n",
    "\n",
    "The following cells use the `QuickFrameMeasurementTask` to compute the position of the brightest source in the field. The data is stored in a structure that is later augmented with additional information (mined from the EFD) that is required to compute the pointing files.\n",
    "\n",
    "The main idea behind processing the data and storing the results in a file is because the process bellow can take quite a while to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qm_config = QuickFrameMeasurementTask.ConfigClass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qm = QuickFrameMeasurementTask(config=qm_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brightest_source_centroid = []\n",
    "\n",
    "for image_name in acq_image_name:\n",
    "    if image_name is None:\n",
    "        continue\n",
    "    _, _, day_obs, seq_num = image_name.split(\"_\")\n",
    "    day_obs = f\"{day_obs[0:4]}-{day_obs[4:6]}-{day_obs[6:8]}\"\n",
    "    exp = await get_image(\n",
    "            dict(dayObs=day_obs, seqNum=int(seq_num)),\n",
    "            datapath=\"/project/shared/auxTel/\",\n",
    "            timeout=10,\n",
    "            runBestEffortIsr=True,\n",
    "        )\n",
    "    result = qm.run(exp)\n",
    "    brightest_source_centroid.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-compute data\n",
    "\n",
    "Each entry in `brightest_source_centroid` contains the centroid of brightest source in the field in pixel coordinates.\n",
    "\n",
    "The data is augmented with the information queried from the EFD; telescope position, hexapod position, AOS offsets and pointing data.\n",
    "These are needed to compute the corrected pointing data information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angle = np.array(elevation) - np.array(rotator_2) + 90.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for indx in range(len(brightest_source_centroid)):\n",
    "    brightest_source_centroid[indx].elevation = elevation[indx]\n",
    "    brightest_source_centroid[indx].rotator_2 = rotator_2[indx]    \n",
    "    brightest_source_centroid[indx].angle = angle[indx]\n",
    "    brightest_source_centroid[indx].point_data = dict([(key, point_data[key][indx]) for key in (\"expectedAzimuth\", \"expectedElevation\", \"measuredAzimuth\", \"measuredElevation\", \"measuredRotator\")])\n",
    "    brightest_source_centroid[indx].hexapod_x = hexapod_x[indx]\n",
    "    brightest_source_centroid[indx].hexapod_y = hexapod_y[indx]\n",
    "    brightest_source_centroid[indx].hexapod_z = hexapod_z[indx]\n",
    "    brightest_source_centroid[indx].aos_offset = dict(x=ataos_correction_x[indx], y=ataos_correction_y[indx], z=ataos_correction_z[indx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store computed data into a pickle file\n",
    "\n",
    "The data is now stored in a pickle file that can later be read and processed as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_pointing_data_path = f\"data/{year}{month:02d}{day:02d}\"\n",
    "out_pointing_data_name = f\"AT_point_data_{year}{month:02d}{day:02d}_tw{time_window:03d}.pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(out_pointing_data_path):\n",
    "    print(f\"Output destination ({out_pointing_data_path}) does not exists, creating directory tree.\")\n",
    "    os.makedirs(out_pointing_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_pointing_data_file = os.path.join(out_pointing_data_path, out_pointing_data_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Writting data to {out_pointing_data_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(out_pointing_data_file, \"wb\") as fp:\n",
    "    pickle.dump(brightest_source_centroid, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End\n",
    "\n",
    "The file is now ready to be analysed with tpoint to produce a new pointing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSST",
   "language": "python",
   "name": "lsst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
